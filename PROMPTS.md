# Initial Prompt, 2025-09-08 1:30 pm SGT

Generated based on https://chatgpt.com/share/68be6b66-0be4-800c-a2e8-d817ce316ccb

---

Goal: A **static** single-page web app (no backend, **localStorage only**) that (1) ingests documents to extract **atomic memory items** (type=`code|llm`), (2) consolidates (delete/merge/edit).

1. Ingests user-uploaded documents (**.txt, .md, .pdf**) in user-editable batche sizes (**default: 5**) to **extract** memory items (in code or llm)

  - Pass only 1 document to LLM per request. Run batches in parallel.
  - Pass PDF files base64-encoded; pass text files as text.
  - LLM generates a `{memories: []}` list (each memory like below) using a JSON schema response_format:
    ```js
    {
      "type": "code" | "llm",
      "rule": "code: JS function source; llm: instruction for how to check",
      "priority": "low" | "medium" | "high",
      "rationale": "why this memory/rule matters",
      "sources": [
        {"quote": "..."}
      ],
    }
    ```
  - The JS code generated should be a `function check(doc) { ... return { pass: true/false, reason: "..." } }`n
  - App adds metadata:
    - id: `mem-1`, `mem-2`, ...
    - updates each item in sources to add `filename`:

2. consolidates memory (delete/merge/edit) if memory count > user-editable threshold (default: 30) or on user clicking on "Consolidate" button. Consolidation pauses ingestion.
  - OpenAI model generates consolidation suggestions passing the LLM a JSON schema:
    ```js
    {
      "deletes": [{ "id": "...", "reason": "duplicate of mem_y" }],
      "edits": [{ "id": "...", "update": { "rule": "…", "priority": 2, "rationale": "…" }, "reason": "clarify scope" }],
      "merges": [
        {
          "from": ["...", "..."],  // 2+ IDs to merge
          "update": { // merged
            "type": "code|llm",
            "rule": ...
            "priority": ...,
            "rationale": ...
          },
          "reason": "generalized rules",
          // app adds new unique ID for merged rule.
          // app merges sources: by concatenating sources
        }
      ]
    }
    ```
  - app applies suggestions and consolidates memory list

This app is a demo of this workflow. Show progress continuously. Specifically:

- As soon as there is an LLM fetch triggered, show a progress indicator
- Show the status of each document as an elegant list, e.g. "queued", "processing", "done", "error: ..."
- Stream the memory items and show them as they get extracted using lit-html.
  - When displaying the memory items, show the memory.rule, and clicking on it should show other details.
- Stream **per-document** results (batch-level streaming). Parse via partial-json and render in real-time.
- Alongside the document status, show the memory list as it gets populated and updated.
- At the bottom, show an elegant collapsible history of actions that shows every step, e.g.
  - Ingestion: For each document, show a summary of the memory items extracted. The collapsed view will show the documents, expanded view will show the memory items under each.
  - Consolidation: Show each consolidation step (deletes, edits, merges) at the top level; expanded view will show details, including reason
  - When displaying the history, show the memory items extracted in the same format as the memory items section, re-using code.

Follow the visual and coding style of these repositories:

- /home/sanand/code/datagen/
- /home/sanand/code/apiagent/
- /home/sanand/code/mindgen/

Follow the same README structure, `index.html` style - updating the existing one (add introduction to the app, advanced settings section which will include batch size, consolidate threshold), and code style as `script.js` from those projects.

This repo will be at https://github.com/sanand0/memlearn and deployed at https://sanand0.github.com/memlearn/

Use asyncLLM for OpenAI responses streaming:

```js
import { asyncLLM } from "https://cdn.jsdelivr.net/npm/asyncllm@2";

const body = {
  model: "gpt-4.1-mini",
  // You MUST enable streaming, else the API will return an {error}
  stream: true,
  input: "Hello, world!",
};

for await (const data of asyncLLM("https://api.openai.com/v1/responses", {
  method: "POST",
  headers: { "Content-Type": "application/json", Authorization: `Bearer ${apiKey}` },
  body: JSON.stringify(body),
})) {
  console.log(data)
}
```

The output will be like:

```
{ content: "Hello", message: { "item_id": "msg_...", ...} }
{ content: "Hello!", message: { "item_id": "msg_...", ...} }
{ content: "Hello! How", message: { "item_id": "msg_...", ...} }
...
{ content: "Hello! How can I assist you today?", message: { "item_id": "msg_...", ...} }
```

In case of an error, there will be a data.error key.

The memories array can be extracted via `parse(content)?.memories`. It will have partial updates to the JSON. For each event generated by asyncLLM, update the UI.
Similarly, for each consolidation event, update the UI.

Here's an example of the body of the Responses API:

```js
{
  model: "...",
  instructions: "..."
  input: [
    {
      role: "user",
      content: [
        {"type": "input_text", "text": "# filename.ext\n\n..."},
        // For PDFs:
        {"type": "input_file", "filename": "...", "file_data": "... base64 ..."}
      ]
    }
  ],
  text: {
    format: {
      type: "json_schema",
      strict: true,
      name: "memories",
      schema: ...
    }
  }
}
```

Generate only these files:

- index.html
- script.js
- package.json
- LICENCE
- config.json (contains schemas)

IMPORTANT: Skip type checking! Skip tests! Just lint via `npm run lint`. This is a prototype.
